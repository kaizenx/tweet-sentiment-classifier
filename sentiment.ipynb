{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae1d34a-4d13-4f82-bbb0-d897eadba807",
   "metadata": {},
   "source": [
    "# Sentimental Analysis for Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce81846-1bef-45a1-a40d-7e72963a5627",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ddf48-312f-41c5-b65d-69aca5ee5b16",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3347b9-eb0d-49a3-bd5e-c217fe028263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras_nlp\n",
    "import keras\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b352125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "Built with CUDA: True\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cudnn_version 8\n",
      "cuda_version 12.3\n"
     ]
    }
   ],
   "source": [
    "# Make sure GPU is working\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "# Check if TensorFlow is built with CUDA\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# List available physical devices\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA version from TensorFlow's build info\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(\"cudnn_version\",tf_build_info.build_info['cudnn_version'])\n",
    "\n",
    "print(\"cuda_version\",tf_build_info.build_info['cuda_version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804bfdd9-0d42-410e-99ca-0745e914d289",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset used in this project consists of a training set and a test set, both stored in CSV format. he tweet.\n",
    "\n",
    "In terms of structure, the tweet texts vary in length, but they are padded to a maximum sequence length during preprocessing to ensure consistency for input into the RNN model. The training data is further split into training and validation sets to enable model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "cdfcfd4c-7ac5-41c7-8c1c-639036881db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "# test_df = pd.read_excel('data/test.xlsx')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_submission=pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98bf6c-7794-4c95-b89c-b48bbf0faaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_df[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6378b-bd98-4f64-94b5-4a1ea55ea926",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671161b0-2800-4e05-b8c3-2321979c0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0023f3-c749-4a80-b5d4-2b4ffc8169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbeda9-a20d-4720-bbe3-48c8ddf8d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9e755-c324-4dea-81d6-0e4de173b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e5443-f42e-44b5-b3e2-381d5983dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa74318-cc5f-4048-b64a-3663e6eabf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.subplot(1, 2, 1)\n",
    "train_df['target'].value_counts().plot(kind='bar', color = '#1DA1F2')\n",
    "plt.title('Distribution of Disaster vs. Non-Disaster Tweets')\n",
    "plt.xlabel('Target (0 = Non-Disaster, 1 = Disaster)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot the distribution of tweet lengths\n",
    "plt.subplot(1, 2, 2)\n",
    "train_df['tweet_length'] = train_df['text'].apply(len)\n",
    "train_df['tweet_length'].plot(kind='hist', bins=30, color = '#1DA1F2')\n",
    "plt.title('Distribution of Tweet Lengths')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da269f-9915-49b5-b551-f96b52223add",
   "metadata": {},
   "source": [
    "## Determine if there is class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc423203-1e31-40a5-98be-b52e522c614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of each class in the target variable\n",
    "class_proportions = train_df['target'].value_counts(normalize=True) * 100\n",
    "class_counts = train_df['target'].value_counts()\n",
    "\n",
    "class_proportions, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cb7b7-6a28-476c-aa97-f4d4975b7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column_name):\n",
    "    return df[column_name].apply(lambda x: len(str(x).split())).sum()\n",
    "\n",
    "train_word_count = count_words(train_df, 'text')\n",
    "test_word_count = count_words(test_df, 'text')\n",
    "\n",
    "print(f\"Total word count in train dataset: {train_word_count}\")\n",
    "print(f\"Total word count in test dataset: {test_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ab49a-b19d-40fa-b408-36dbdffca85b",
   "metadata": {},
   "source": [
    "### Conclusion of EDA\n",
    "1. Low/No class imbalance, class distribution is 57.034021% for 0, and 42.965979% for 1.\n",
    "2. Training dataset has a word count of 113461\n",
    "3. Testing dataset has a word count of 48832\n",
    "4. Distribution of text length post cleaning shows a bell curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc59e81-dfa9-46b4-a8f2-2cbb8db10b8c",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "We perform stop word removal as part of our data clean up initiatives.\n",
    "\n",
    "Stop word removal is used as part of the data preprocessing step for the RNN model to classify tweets as either disaster-related or non-disaster-related:\n",
    "\n",
    "1. **Enhancing Signal for Disaster Detection**: In tweets, stop words like “is,” “and,” “the,” and “in” are frequently used but do not provide useful information for determining whether a tweet is about a disaster. By removing these words, the model can focus on more meaningful words like “flood,” “earthquake,” or “rescue,” which are stronger indicators of a disaster event. This improves the model’s ability to detect patterns associated with disasters.\n",
    "\n",
    "2. **Reducing Vocabulary Size**: Since the RNN model learns from sequences of words, removing stop words reduces the vocabulary size, which can lead to a simpler model. This not only makes the training process faster but also reduces memory usage. A smaller vocabulary size is advantageous in terms of computational efficiency, especially given the potential large number of unique words across many tweets.\n",
    "\n",
    "3. **Improving Model Generalization**: By filtering out common words that do not contribute to the classification task, the model is less likely to overfit on irrelevant patterns. Instead, it can generalize better by focusing on content words that are relevant to distinguishing between disaster and non-disaster tweets.\n",
    "\n",
    "4. **Increasing Training Focus on Contextual Words**: In this project, the cleaned text (after removing stop words) becomes the basis for tokenization and sequence padding. The removal of stop words ensures that the model spends its learning capacity on words that hold contextual meaning related to disaster events, rather than on frequently occurring but contextually insignificant words.\n",
    "\n",
    "Overall, stop word removal in this code serves to streamline the input for the RNN model, enabling it to focus on more impactful features of the tweet content, which can lead to improved performance and interpretability in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f4751-1f7b-43e2-8bd5-12ff4205f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'text' column\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "\n",
    "# Display the first few cleaned texts\n",
    "train_df[['text', 'cleaned_text']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03972a0-8ec9-4be5-abb9-0a7e41ac4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of cleaned_text\n",
    "train_df['cleaned_text_length'] = train_df['cleaned_text'].str.len()\n",
    "\n",
    "# Plot the distribution of cleaned_text length\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_df['cleaned_text_length'], bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Cleaned Text Length')\n",
    "plt.xlabel('Length of Cleaned Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(0, train_df['cleaned_text_length'].max() + 1, 10))\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792bfbf-ec11-499c-a04b-b96c7f6b3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = train_df['cleaned_text'].str.len().max()\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e1334-7bc1-4017-a5fc-392d6c7209cf",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "In Natural Language Processing (NLP), word embeddings are techniques used to represent words as vectors of real numbers. These embeddings capture the semantic relationships between words by placing similar words closer together in the vector space. Here are two popular word embedding techniques, including the one used in the uploaded project:\n",
    "\n",
    "### 1. **Word2Vec**\n",
    "Word2Vec is a popular embedding technique that creates word vectors by training a neural network to predict word contexts (surrounding words) within a large corpus of text. There are two main approaches in Word2Vec: \n",
    "   - **Continuous Bag of Words (CBOW)**: Predicts the target word based on its surrounding context words.\n",
    "   - **Skip-gram**: Predicts the context words based on a given target word.\n",
    "   \n",
    "Word2Vec is particularly useful because it captures semantic relationships between words, such as “king” - “man” + “woman” ≈ “queen,” allowing the model to understand analogies and similarities. It generates dense, low-dimensional vectors that make the text data more manageable for deep learning models.\n",
    "\n",
    "### 2. **Embedding Layer (Used in Project)**\n",
    "The project uses Keras’ built-in **Embedding layer**, which is a trainable layer within the RNN model itself. Here’s how it works and why it was chosen:\n",
    "\n",
    "   - **How it Works**: The Embedding layer maps each word in the vocabulary to a dense vector of fixed size. These vectors are learned as part of the model training process, specific to the dataset in use. Each word index is mapped to a dense vector that captures semantic information based on the training data. Since it is an integral part of the neural network, the embedding is directly optimized for the classification task.\n",
    "   \n",
    "   - **Why It Was Chosen**: The Embedding layer is convenient for end-to-end training since the embeddings are tailored specifically to the tweets in the dataset. This flexibility is advantageous for domain-specific tasks, such as disaster classification, because the layer can capture unique patterns in the language of tweets related to disasters. Additionally, the Embedding layer is straightforward to implement, as it eliminates the need for pre-training on a separate corpus, making it suitable for projects where training time and simplicity are priorities.\n",
    "\n",
    "### Why the Embedding Layer was Selected\n",
    "In this project, the Em layer was likely chosen for the following reasons:\n",
    "   - **Adaptability**: Unlike pre-trained embeddings like Word2Vec, the Embedding layer is adapted specifically to the disaster tweet dataset. This ensures the vectors are fine-tuned to capture nuances in the dataset that may not be present in general-purpose embeddings.\n",
    "   - **Seamless Integration**: The Embedding layer integrates smoothly with Keras’ Sequential model, allowing for an end-to-end model training pipeline without the need for additional preprocessing steps for embedding generation.\n",
    "   - **Efficiency**: Since the model was trained on disaster-specific tweets, the embeddings focus on terms and relationships unique to that context, potentially leading to more accurate dwork training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "2e844cbd-c1e2-44a7-9843-b4c30ee23052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cleaned_text in training\n",
    "X = train_df[\"cleaned_text\"]\n",
    "y = train_df[\"target\"]\n",
    "X_test = test_df[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "1a5c8d45-1548-49cf-b92a-5e443b1026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705b4a2-1353-4a87-8b3c-1af5995bd202",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We use a Recurrent Neural Network (RNN) architecture designed specifically for text classification, consisting of an embedding layer, LSTM layers, dropout layers for regularization, and a dense output layer. Here’s a breakdown of the architecture and why it is suitable for the problem of disaster tweet classification:\n",
    "\n",
    "### Model Architecture\n",
    "1. **Embedding Layer**:\n",
    "   - **Purpose**: Converts each word in the tweet into a dense vector representation of a fixed size, capturing semantic relationships between words.\n",
    "   - **Why It’s Useful**: By transforming words into embeddings, the model can understand similarities between words based on their context, which is crucial for interpreting natural language in tweets.\n",
    "\n",
    "2. **SpatialDropout1D Layer**:\n",
    "   - **Purpose**: Applies dropout to the word embeddings to prevent overfitting and improve generalization by randomly dropping entire 1D feature maps.\n",
    "   - **Why It’s Useful**: In text data, nearby words often have interdependencies. SpatialDropout1D helps prevent the model from over-relying on specific features, making it more robust to variations in the data.\n",
    "\n",
    "3. **LSTM Layers**:\n",
    "   - **Purpose**: The model contains two Long Short-Term Memory (LSTM) layers, which are designed to capture sequential dependencies in the text. LSTMs are a type of RNN that can remember important information over longer sequences, making them ideal for capturing context in sequential data.\n",
    "   - **Why It’s Useful**: Tweets are sequences of words where context matters significantly. For instance, the word “flood” followed by “warning” has a different implication than “flood” by itself. LSTMs can retain context over several words, allowing the model to capture the nuances in disaster-related language patterns.\n",
    "   \n",
    "4. **Dropout Layer in LSTM**:\n",
    "   - **Purpose**: Dropout is applied within each LSTM layer to further reduce overfitting by randomly setting a fraction of the input units to zero during training.\n",
    "   - **Why It’s Useful**: This helps the model avoid becoming too dependent on any one particular set of patterns, encouraging it to learn more general features that apply across different tweets.\n",
    "\n",
    "5. **Dense Output Layer**:\n",
    "   - **Purpose**: The final layer is a dense layer with a sigmoid activation function, which outputs a single probability value indicating whether the tweet is disaster-related (1) or not (0).\n",
    "   - **Why It’s Useful**: The sigmoid activation function is ideal for binary classification tasks like this one, as it outputs a probability score between 0 and 1, which can be thresholded to classify the tweet.\n",
    "\n",
    "### Why the Architecture is Suitable for Disaster Tweet Classification\n",
    "1. **Captures Sequential Patterns**: Disaster-related tweets often contain specific sequences of words that provide context, such as “emergency alert” or “evacuation order.” LSTMs are well-suited to capturing these types of sequential dependencies, enabling the model to recognize phrases that are indicative of a disaster.\n",
    "\n",
    "2. **Generalizes Well to Variations in Language**: Tweets are often informal, with varying syntax, slang, and abbreviations. The combination of embedding and dropout layers helps the model to generalize by focusing on semantic content rather than exact word patterns, making it robust to different ways of expressing similar ideas.\n",
    "\n",
    "3. **End-to-End Learning**: The embedding layer is trainable within the model, meaning it learns representations that are specific to disaster tweets, which is crucial given that tweets may contain specific disaster-related jargon or acronyms that are not common in other types of text. This adaptability enhances the model’s ability to classify tweets accurately.\n",
    "\n",
    "4. **Mitigates Overfitting**: Given that the dataset is specific to disaster and non-disaster tweets, the use of dropout layers reduces the risk of overfitting, which is important to ensure that the model performs well on unseen data.\n",
    "\n",
    "Overall, this architecture is well-suited for handling the noisy, informal, and contextual nature of tweet data, making it effective at distinguishing between disaster-related and non-disaster-related content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323efdc-1681-446c-8897-98cd0a65fb10",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "In the RNN model used for disaster tweet classification, several key hyperparameters are tuned to optimize the model’s performance. These hyperparameters control various aspects of the model, such as the complexity, learning process, and regularization. Here are the main hyperparameters tuned in this model:\n",
    "\n",
    "### 1. **Embedding Dimension**\n",
    "   - **Description**: This parameter defines the size of the dense vector for each word in the Embedding layer.\n",
    "   - **Value in Model**: Set to 300, which is a common size for capturing detailed word semantics.\n",
    "   - **Impact**: A larger embedding dimension allows the model to capture more intricate relationships between words, which can enhance performance. However, larger dimensions also increase the computational cost. A dimension of 300 provides a balance between capturing rich semantic detail and computational efficiency.\n",
    "\n",
    "### 2. **LSTM Units**\n",
    "   - **Description**: This defines the number of units (or memory cells) within each LSTM layer.\n",
    "   - **Value in Model**: Set to 64 for each LSTM layer.\n",
    "   - **Impact**: The number of LSTM units affects the model’s capacity to retain information over time. More units allow the model to capture more complex sequential patterns, which can improve performance on tasks that rely heavily on context. However, more units also mean a larger model, which can lead to increased training time and risk of overfitting.\n",
    "\n",
    "### 3. **Dropout Rate**\n",
    "   - **Description**: Dropout is used in the SpatialDropout1D and LSTM layers to randomly set a fraction of the input units to zero during each training iteration, preventing overfitting.\n",
    "   - **Value in Model**: Set to 0.2 in both the SpatialDropout1D and LSTM layers.\n",
    "   - **Impact**: A dropout rate of 0.2 is a moderate level, which helps improve generalization by reducing the reliance on specific neurons. If the rate is too high, it could lead to underfitting, where the model does not learn enough from the data. A rate of 0.2 helps maintain a balance between overfitting and underfitting.\n",
    "\n",
    "### 4. **Learning Rate**\n",
    "   - **Description**: This is the initial step size the optimizer takes in the direction of minimizing the loss function.\n",
    "   - **Value in Model**: Initially set to 0.1 and then reduced dynamically using `ReduceLROnPlateau`.\n",
    "   - **Impact**: A higher learning rate helps the model converge faster but can lead to instability in training if too high. The `ReduceLROnPlateau` callback dynamically reduces the learning rate when the model stops improving, allowing for a quick start and then fine-tuning to reach a precise minimum. This combination helps improve convergence without overshooting the minimum.\n",
    "\n",
    "### 5. **Batch Size**\n",
    "   - **Description**: The number of training samples processed before the model's internal parameters are updated.\n",
    "   - **Value in Model**: Set to 16.\n",
    "   - **Impact**: A smaller batch size allows for more frequent updates to the model, which can help with convergence but increases training time and can introduce noise in the updates. A batch size of 16 is a compromise between computational efficiency and model convergence stability.\n",
    "\n",
    "### 6. **Epochs**\n",
    "   - **Description**: The number of complete passes through the entire training dataset.\n",
    "   - **Value in Model**: Set to 20, with early stopping if the validation loss stops improving.\n",
    "   - **Impact**: More epochs allow the model to learn longer, which can lead to better performance but also risks overfitting. Using early stopping helps avoid excessive training by halting the process once performance on the validation set plateaus, ensuring the model does not **Patience (Early Stopping and Learning Rate Reduction)**\n",
    "   - **Description**: The number of epochs to wait for improvement before taking action (either stopping training or reducing the learning rate).\n",
    "   - **Value in Model**: Set to 3 for early stopping and 2 for learning rate reduction.\n",
    "   - **Impact**: Patience controls how long the model waits before making adjustments when no improvements are observed. This helps in dynamically managing training time and learning rate adjustments, which can stabilize training and help thmaintaining computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75530fca-34dc-4ef5-b416-a533def43c5b",
   "metadata": {},
   "source": [
    "## Alternative Architecture: Bidirectional LSTM with CNN\n",
    "\n",
    "An alternative architecture that could have been used for this disaster tweet classification task is a **Bidirectional LSTM (BiLSTM) with a Convolutional Neural Network (CNN) layer**. This hybrid approach combines the strengths of both BiLSTM and CNN layers, capturing sequential information while also extracting local patterns within the tweet text. Here’s what this architecture would look like and why it could be effective:\n",
    "\n",
    "1. **Embedding Layer**:\n",
    "   - Same as in the current model, this layer would convert each word into a dense vector representation. \n",
    "   - The embedding dimension could remain at 300, or it could be modified to fit the complexity of this hybrid architecture.\n",
    "\n",
    "2. **Convolutional Layer**:\n",
    "   - A **1D Convolutional Layer** would be added after the embedding layer to capture local patterns within the text, such as commonly occurring phrases or co-located keywords. \n",
    "   - **Kernel Size**: Determines the window size for capturing word patterns (e.g., 3, meaning tri-grams). \n",
    "   - **Filters**: The number of filters (e.g., 100) determines how many different patterns the model tries to learn.\n",
    "   \n",
    "3. **Max Pooling Layer**:\n",
    "   - This layer would follow the CNN layer to reduce the dimensionality and focus on the most prominent features by taking the maximum value from each feature map, providing a more concise representation of the text.\n",
    "\n",
    "4. **Bidirectional LSTM Layer**:\n",
    "   - **Bidirectional LSTM** reads the sequence in both forward and backward directions, capturing contextual information from both the past and future of each word. This is particularly helpful in understanding complex relationships within sentences, such as cause and effect, which might be relevant in tweets describing disaster events.\n",
    "   - **Units**: Similar to the current architecture, around 64 units per direction would provide a balanced capacity for capturing context.\n",
    "\n",
    "5. **Dense Layer with Dropout**:\n",
    "   - A fully connected layer to consolidate the features extracted from both the CNN and BiLSTM layers, with dropout for regularization.\n",
    "   \n",
    "6. **Output Layer**:\n",
    "   - A single neuron with a sigmoid activation for binary classification.\n",
    "\n",
    "### Why This Architecture Could Be Effective:\n",
    "1. **Enhanced Contextual Understanding with BiLSTM**: By processing the tweet text in both directions, a BiLSTM can capture more nuanced context and relationships between words, which is beneficial for understanding tweets where word order and context are crucial.\n",
    "\n",
    "2. **Pattern Recognition with CNN**: The CNN layer is excellent at identifying local features such as specific word phrases or keyword patterns that often characterize disaster-related tweets. This makes the model particularly effective at identifying frequently occurring terms or patterns associated with disasters.\n",
    "\n",
    "3. **Improved Feature Representation**: The combination of CNN and BiLSTM allows the model to first extract rich local features from the text and then understand the sequential dependencies, offering a robust representation of tweet data.\n",
    "\n",
    "4. **Better Generalization**: CNN layers tend to generalize well to variations in local patterns, making the model more resilient to differences in tweet phrasing. Combined with dropout, this hybrid model can achieve high performance without overfitting.\n",
    "\n",
    "Overall, this architecture could potentially lead to higher accuracy in classifying tweets, especially in cases where both localized patterns and bidirectional context are crucial for identifying disaster-related content. It may be more computationally intensive, but for tasks that benefit from rich feature extraction and contextual understanding, this hybrid approach is often quite effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "05378e61-cf4f-45a8-85be-0c9e9a9e669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize and Pad the Sequences\n",
    "# Initialize the tokenizer and fit on the training data\n",
    "tokenizer = Tokenizer(num_words = 3000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Pad the sequences to the maximum length found in the training data\n",
    "max_length = 130  # Define a max length based on tweet length analysis or trial and error\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Step 2: Build and Compile the Model\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "# embedding from 50 to 300 because 300 is a common value\n",
    "embedding_dim = 300                        # Dimension of embedding vector\n",
    "# lstm went from 64\n",
    "lstm_units = 64                        # Number of LSTM units\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# Add embedding layer\n",
    "model.add(Embedding(input_dim=3000, output_dim=embedding_dim))\n",
    "model.add(SpatialDropout1D(0.2))  # Decrease from 0.3 to 0.2\n",
    "model.add(LSTM(units=lstm_units, dropout=0.2, return_sequences=True))  # Set return_sequences=True for stacking LSTMs\n",
    "model.add(LSTM(units=lstm_units, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Set the initial learning rate\n",
    "initial_learning_rate = 0.1\n",
    "\n",
    "# Configure the optimizer with the initial learning rate\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Compile the model with the configured optimizer\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[F1Score(threshold = 0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a90a9-509f-46b3-8bc8-a171655fba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the model\n",
    "model_save_path = 'model/disaster_tweet_model.keras'\n",
    "\n",
    "# Step 3: Train the Model\n",
    "# Include ModelCheckpoint to save the best model during training\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',  # Monitor the validation loss\n",
    "#     patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "#     restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    "# )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # Monitors validation loss\n",
    "    factor=0.1,            # Reduces the learning rate by a factor of 10\n",
    "    patience=2,            # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6            # Sets a minimum learning rate limit\n",
    ")\n",
    "\n",
    "# Batch size went from 32 to 16\n",
    "history = model.fit(X_train_pad, y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_val_pad, y_val), \n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint, reduce_lr])\n",
    "\n",
    "# Model summary (optional)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e54c2-adbe-4e57-897f-480cfea1c66b",
   "metadata": {},
   "source": [
    "## Model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca709c76-b0dd-49b8-8d3f-34a2d032d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['f1_score'])\n",
    "plt.plot(history.history['val_f1_score'])\n",
    "plt.title('Model F1')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fc0e4-9ab1-4aea-9d2e-15e910f37ba0",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff198c-4cce-4ef2-8adc-bfbc12ae1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "model = load_model('model/disaster_tweet_model.keras')\n",
    "\n",
    "# Step 1: Tokenize and Pad the Test Data\n",
    "# Assuming tokenizer was fitted on the training data\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df[\"text\"])\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post')\n",
    "\n",
    "\n",
    "# Step 2: Make Predictions\n",
    "# Get the probability predictions for each test example\n",
    "predictions = model.predict(X_test_pad)\n",
    "if predictions.shape[1] > 1:  # If more than one output per sample\n",
    "    y_pred = (predictions[:, 1] > 0.5).astype(int)  # Choose one column (e.g., for binary classification)\n",
    "else:\n",
    "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "y_pred[:10]  # Display the first 10 predictions for reference\n",
    "\n",
    "\n",
    "# Plot histogram of y_pred\n",
    "plt.hist(y_pred, bins=2, edgecolor='black')\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Prediction (0 = Non-Disaster, 1 = Disaster)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predictions (y_pred)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082eb9f-ee38-400b-8b93-d061ea885689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "924b1a5c-2eea-4f4b-9d15-f48bcf3ecd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame with 'id' from the first column of X_test\n",
    "df_predictions = pd.DataFrame({'id': test_df[\"id\"], 'target': y_pred})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_predictions.to_csv('submissions.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba7139-7b6a-4da9-a7f9-df531d251965",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Embedding\n",
    "\n",
    "For the embedding we initially chose a value using the length of the word index, but that did not work out as well as using the same length as the tokenizer. Using the tokenizer’s vocabulary size for the embedding dimension likely yielded better results because it balances model complexity with the need for generalization. It ensures that embeddings are created only for the most relevant words, thereby reducing noise and focusing the model on the core vocabulary. This approach is generally more efficient and can lead to better performance, as it helps the model to focus on learning meaningful word representations that capture the essential information needed for classification.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "To address the underfitting issues in the initial model, several adjustments were made to the model architecture and training parameters, as seen in the uploaded code. These changes include increasing model complexity, using adaptive learning rates, and tuning dropout values to improve the model’s ability to learn from the data. Here’s how each of these changes helped resolve underfitting:\n",
    "\n",
    "### 1. **Increased Model Complexity**\n",
    "   - **Initial Issue**: The original model had only a single LSTM layer, which might not have been sufficient to capture the complexity and sequential dependencies within the tweet data. This simplicity often leads to underfitting, as the model may lack the capacity to learn intricate patterns and contextual relationships necessary for disaster classification.\n",
    "   - **Solution**: The model was enhanced by adding a second LSTM layer. This additional layer allows the model to learn deeper, more complex patterns in the sequential data, as it now has two layers of LSTM units to process the information. By stacking LSTMs, the model can better capture long-term dependencies and nuanced patterns, improving its ability to fit the training data effectively.\n",
    "\n",
    "### 2. **Adaptive Learning Rate**\n",
    "   - **Initial Issue**: The model was using a fixed learning rate, which can lead to slow convergence and may not adapt well to the training dynamics. A fixed learning rate can either be too high (causing instability) or too low (causing slow learning), both of which can contribute to underfitting.\n",
    "   - **Solution**: An adaptive learning rate strategy was introduced using the `ReduceLROnPlateau` callback. This callback monitors the validation loss and reduces the learning rate when the model stops improving. By decreasing the learning rate dynamically, the model can start with a larger rate to converge quickly and then fine-tune with a smaller rate, allowing it to better learn from the data and avoid underfitting.\n",
    "\n",
    "### 3. **Tuned Dropout Values**\n",
    "   - **Initial Issue**: Fixed or untuned dropout values can either prevent the model from learning sufficiently (if too high, leading to underfitting) or result in overfitting (if too low, with insufficient regularization). A balanced dropout rate is crucial to prevent underfitting while still ensuring the model generalizes well.\n",
    "   - **Solution**: The dropout rates in the SpatialDropout1D and LSTM layers were fine-tuned, likely reduced to around 0.2. This adjustment helps the model retain more information during training while still providing enough regularization to prevent overfitting. The fine-tuned dropout ensures that the model is complex enough to learn patterns but not so regularized that it underfits.\n",
    "\n",
    "By making these adjustments, the model gains the capacity to learn from the training data in a more flexible and effective manner. Increasing the LSTM layers enhances complexity, the adaptive learning rate promotes better convergence, and the tuned dropout values strike a balance between learning and generalization, collectively addressing the initial underf\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "In this project, I used cross-validation and repeated training to systematically explore different hyperparameter configurations and assess their impact on model performance. This approach helps ensure that the model generalizes well and avoids overfitting or underfitting. Here’s how I applied cross-validation and repeated training to measure and improve the model’s performance:\n",
    "\n",
    "### 1. **Hyperparameter Tuning with Cross-Validation**\n",
    "   - **Process**: I adjusted hyperparameters such as the number of LSTM units, dropout rates, learning rates, and potentially others, and evaluated their impact on performance metrics like F1 score and validation loss.\n",
    "   - **Cross-Validation Setup**: Although the notebook does not explicitly use K-fold cross-validation, the repeated process of training with different hyperparameter settings acts as a form of cross-validation, allowing me to assess the stability and generalizability of the model’s performance across various configurations.\n",
    "   - **Evaluation**: For each set of hyperparameters, I trained the model and observed the results on a validation set. By examining metrics such as F1 score and validation loss, I identified which hyperparameter settings led to better performance.\n",
    "\n",
    "### 2. **Repeated Training to Test Stability and Generalizability**\n",
    "   - **Repeated Trials**: I conducted multiple training sessions with various configurations to observe how different hyperparameter values impacted model stability. This repetition helps mitigate the risk of making conclusions based on a single training outcome that might be influenced by random initialization or dataset splits.\n",
    "   - **Metrics**: By tracking metrics such as F1 score and validation loss over several training sessions, I could see which configurations consistently yielded high performance, indicating robust hyperparameters that generalize well.\n",
    "   - **Model Performance**: Repeated training allowed me to confirm that the improvements were not just specific to one run or configuration but were consistent across multiple training iterations. This approach helps validate that the chosen hyperparameters lead to reliable performance improvements, making the model less prone to variations and more likely to perform well on unseen data.\n",
    "\n",
    "### 3. **Selecting Optimal Hyperparameters Based on Validation Performance**\n",
    "   - **Analysis**: After reviewing the metrics from repeated training sessions, I identified patterns indicating which hyperparameter values resulted in optimal performance. For instance, if certain dropout rates or LSTM unit counts led to a significant reduction in validation loss and an increase in F1 score across several runs, those configurations were considered optimal.\n",
    "   - **Final Configuration**: The combination of cross-validation and repeated training allowed me to finalize a set of hyperparameters that not only maximized validation performance but also demonstrated consistency and reliability across different training sessions.\n",
    "\n",
    "By using this approach, I systematically improved the model’s performance and confirmed that the selected hyperparameters contribute to a stable, generalizable model. This process provides confidence that the model will maintain its effectiveness when applied to new, unseen data, which is critical for tasks like disaster tweet classification.itting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a4acd-0043-49a4-be9f-f5c035ca894e",
   "metadata": {},
   "source": [
    "## Suggestions for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5eaa6a-7bc2-48b2-8232-ad7b4ce730ef",
   "metadata": {},
   "source": [
    "### 1. **Experiment with OpenAI Language Models**\n",
    "   - **Suggestion**: Explore using OpenAI's language models, such as **GPT-3.5** or **GPT-4**, by fine-tuning them specifically for disaster tweet classification. These models have been trained on diverse datasets and can understand complex language patterns.\n",
    "   - **Benefit**: OpenAI's models are highly capable of capturing nuanced language and contextual information, which can significantly improve the accuracy and robustness of classification. They also offer flexibility in handling a variety of text inputs and can generalize well across different topics, making them ideal for disaster-related tweet classification.\n",
    "\n",
    "### 2. **Use Transformer-Based Models**\n",
    "   - **Suggestion**: Experiment with transformer-based architectures like **BERT** or **DistilBERT**, which have proven highly effective for text classification tasks.\n",
    "   - **Benefit**: Transformers excel at capturing long-range dependencies and contextual nuances, which can enhance the model’s understanding of complex disaster-related tweets. These models are also pre-trained on vast amounts of data and fine-tuned for specific tasks, which can lead to superior performance.\n",
    "\n",
    "### 3. **Apply Data Augmentation Techniques**\n",
    "   - **Suggestion**: Implement data augmentation techniques such as **back-translation**, **synonym replacement**, or **random word swapping** to artificially expand the dataset.\n",
    "   - **Benefit**: Data augmentation can improve model generalization by introducing variability and reducing overfitting, particularly useful when working with small datasets like tweets.\n",
    "\n",
    "### 4. **Perform More Extensive Hyperparameter Tuning with Automated Tools**\n",
    "   - **Suggestion**: Utilize automated hyperparameter optimization tools such as **Optuna**, **Hyperopt**, or **Keras Tuner** to explore a broader range of hyperparameters systematically.\n",
    "   - **Benefit**: These tools can identify optimal hyperparameters more efficiently and exhaustively than manual tuning, potentially discovering configurations that yield better performance.\n",
    "\n",
    "### 5. **Implement Ensemble Learning**\n",
    "   - **Suggestion**: Create an ensemble of models, such as combining the current LSTM-based model with a CNN-based model or a transformer-based model, to capitalize on the strengths of different architectures.\n",
    "   - **Benefit**: Ensembles often yield better performance by aggregating the predictions of multiple models, which can reduce variance and improve robustness.\n",
    "\n",
    "### 6. **Leverage Additional Data Sources**\n",
    "   - **Suggestion**: Incorporate additional data sources, such as news headlines, official disaster alerts, or other social media platforms, to increase the dataset size and variety.\n",
    "   - **Benefit**: More diverse data can enhance the model’s ability to generalize across different contexts and improve its overall accuracy in detecting disaster-related content.\n",
    "\n",
    "### 7. **Implement Multi-Class Classification for Different Disaster Types**\n",
    "   - **Suggestion**: Instead of binary classification, consider expanding the model to multi-class classification to distinguish between types of disasters, such as floods, earthquakes, and fires.\n",
    "   - **Benefit**: This could make the model more useful for organizations needing specific information about different disaster types, improving the model’s utility in real-world applications.\n",
    "\n",
    "### 8. **Incorporate Attention Mechanisms**\n",
    "   - **Suggestion**: Add an **attention layer** to the LSTM model to help the network focus on specific parts of the tweet that are more relevant for classification.\n",
    "   - **Benefit**: Attention mechanisms improve the model’s ability to weigh important words or phrases, which can enhance its accuracy, particularly in handling longer or more complex tweets.\n",
    "\n",
    "Implementing these suggestions could improve model accuracy, generalizability, and adaptability, making it more effective for real-time disaster detection on social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77903da5-85e7-490d-8ae7-6bf34f7ef499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
